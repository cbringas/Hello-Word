{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/SNC4t720S5f38FlgCzIW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"fz-1PSBcQDii"},"outputs":[],"source":["# --- Montar Google Drive ---\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# --- 1. Instalación e Importación de Librerías ---\n","!pip install yfinance optuna tensorflow scikit-learn matplotlib seaborn pandas numpy\n","!pip install optuna-integration[tfkeras]\n","\n","import pandas as pd\n","import numpy as np\n","import yfinance as yf\n","from datetime import datetime, timedelta\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import tensorflow as tf\n","tf.config.run_functions_eagerly(True)\n","tf.data.experimental.enable_debug_mode()\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from optuna.integration import TFKerasPruningCallback\n","import optuna\n","from google.colab import files\n","import joblib\n","import sys\n","\n","sns.set_style(\"whitegrid\")\n","plt.rcParams['figure.figsize'] = (16, 5)\n","plt.rcParams['lines.linewidth'] = 2\n"],"metadata":{"id":"ESFKP1aMQLDy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 2. Carga del Archivo de la PC (Excel o CSV) ---\n","#print(\"Por favor, sube tu archivo de datos (ej. 'limpio.xlsx' o 'limpio.csv') ahora.\")\n","#uploaded = files.upload()\n","#file_name = list(uploaded.keys())[0]\n","file_path_in_drive = '/content/drive/MyDrive/MP_20251/Datos_MediaMovil.xlsx'\n","try:\n","    # ✅ Cargar el archivo sin especificar columna de fecha como índice\n","    df = pd.read_excel(file_path_in_drive)\n","\n","    # ✅ Mostrar resumen del DataFrame\n","    print(f\"\\n✅ Archivo '{file_path_in_drive}' cargado exitosamente desde Google Drive.\")\n","    print(\"\\n📌 Primeras 5 filas del DataFrame:\")\n","    print(df.head())\n","    print(\"\\n📌 Últimas 5 filas del DataFrame:\")\n","    print(df.tail())\n","    print(\"\\n📌 Información del DataFrame:\")\n","    df.info()\n","\n","    # ✅ Verificar si hay valores NaN\n","    nan_counts = df.isna().sum()\n","    if nan_counts.sum() == 0:\n","        print(\"\\n✅ No se encontraron valores faltantes (NaN) en el DataFrame.\")\n","    else:\n","        print(\"\\n⚠️ Valores faltantes detectados por columna:\")\n","        print(nan_counts[nan_counts > 0])\n","\n","except Exception as e:\n","    print(f\"\\n❌ Ocurrió un error al cargar el archivo: {e}\")\n"],"metadata":{"id":"cddSWUSFQPiR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 4. Preparación de Características (Features) y Variable Objetivo (Target) ---\n","\n","df['PM10_SA_IMP_hORA_Next'] = df['PM10_SA_IMP'].shift(-1) # Para 1 hora adelante. Cambia a .shift(-3) para 3 horas adelante.\n","\n","#df['año'] = df.index.year\n","#df['mes'] = df.index.month\n","#df['dia_del_mes'] = df.index.day\n","#df['dia_de_la_semana'] = df.index.dayofweek\n","#df['hora'] = df.index.hour\n","\n","features = [\n","    'PM10_SA_IMP', 'PM2_5_SA_IMP', 'PM10_SJL_IMP',\n","    'MA_PM10_SA_IMP_24h', 'MA_PM10_SA_IMP_6h',\n","    'MA_PM10_SJL_IMP_24h', 'MA_PM10_SJL_IMP_6h',\n","    'MA_PM2_5_SA_IMP_24h', 'MA_PM2_5_SA_IMP_6h'\n","\n","]\n","\n","missing_features = [f for f in features if f not in df.columns]\n","if missing_features:\n","    print(f\"\\nError: Las siguientes columnas de características no se encontraron en el archivo cargado: {missing_features}\")\n","    print(\"Por favor, verifica que los nombres de las columnas en tu archivo coincidan con los esperados.\")\n","    sys.exit(\"¡Error crítico: Faltan características necesarias en el DataFrame!\")\n","\n","X = df[features]\n","y = df['PM10_SA_IMP_hORA_Next']\n","\n","combined_df = pd.concat([X, y], axis=1).dropna()\n","X_clean = combined_df[features].copy()\n","y_clean = combined_df['PM10_SA_IMP_hORA_Next'].copy()\n","\n","\n","print(f\"\\nDimensiones de X_clean después de la preparación final: {X_clean.shape}\")\n","print(f\"Dimensiones de y_clean después de la preparación final: {y_clean.shape}\")\n"],"metadata":{"id":"UGwgmQbAQUqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# --- 5. División de Datos (75% Train, 25% Test) ---\n","train_size = int(len(X_clean) * 0.75)\n","X_train_df, X_test_df = X_clean.iloc[:train_size], X_clean.iloc[train_size:]\n","y_train_df, y_test_df = y_clean.iloc[:train_size], y_clean.iloc[train_size:]\n","\n","print(f\"\\nDimensiones de X_train_df: {X_train_df.shape}, y_train_df: {y_train_df.shape}\")\n","print(f\"Dimensiones de X_test_df: {X_test_df.shape}, y_test_df: {y_test_df.shape}\")\n","\n","# --- 6. Escalado de Datos (MinMaxScaler) ---\n","scaler_X = MinMaxScaler()\n","scaler_y = MinMaxScaler()\n","\n","X_train_scaled = scaler_X.fit_transform(X_train_df)\n","X_test_scaled = scaler_X.transform(X_test_df)\n","\n","y_train_scaled = scaler_y.fit_transform(y_train_df.values.reshape(-1, 1))\n","y_test_scaled = scaler_y.transform(y_test_df.values.reshape(-1, 1))\n","\n","print(\"\\nDatos escalados exitosamente.\")\n"],"metadata":{"id":"qgZ41fHIQZz5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import optuna\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.metrics import mean_absolute_error\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import LSTM, Dropout, Dense\n","from tensorflow.keras.optimizers import Adam\n","\n","# --- Construcción del modelo LSTM parametrizable ---\n","def build_lstm_model(input_shape, n_layers, n_neurons, dropout, learning_rate):\n","    model = Sequential()\n","    model.add(LSTM(n_neurons, return_sequences=(n_layers > 1), input_shape=input_shape))\n","    model.add(Dropout(dropout))\n","\n","    for i in range(1, n_layers):\n","        model.add(LSTM(n_neurons, return_sequences=(i < n_layers - 1)))\n","        model.add(Dropout(dropout))\n","\n","    model.add(Dense(1))\n","    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n","    return model\n","\n","# --- Preparar datos en secuencias para LSTM ---\n","LOOKBACK_WINDOW = 10  # Últimas 30 horas como entrada\n","\n","def create_lstm_sequences(X_data, y_data, lookback):\n","    X_seq, y_seq = [], []\n","    for i in range(len(X_data) - lookback):\n","        X_seq.append(X_data[i:(i + lookback), :])\n","        y_seq.append(y_data[i + lookback])\n","    return np.array(X_seq), np.array(y_seq)\n","\n","X_train_seq, y_train_seq = create_lstm_sequences(X_train_scaled, y_train_scaled, LOOKBACK_WINDOW)\n","X_test_seq, y_test_seq = create_lstm_sequences(X_test_scaled, y_test_scaled, LOOKBACK_WINDOW)\n","\n","# --- Objective Function para Optuna ---\n","def objective_lstm(trial):\n","    n_layers = trial.suggest_int('n_layers', 1, 2)\n","    n_neurons = trial.suggest_int('n_neurons_per_layer', 32, 128, step=32)\n","    dropout = trial.suggest_float('dropout_rate', 0.1, 0.4, step=0.1)\n","    learning_rate = trial.suggest_float('learning_rate', 5e-4, 5e-3, log=True)\n","    batch_size = trial.suggest_categorical('batch_size', [64, 128])\n","    epochs = 10\n","\n","    model = build_lstm_model(\n","        input_shape=X_train_seq.shape[1:],\n","        n_layers=n_layers,\n","        n_neurons=n_neurons,\n","        dropout=dropout,\n","        learning_rate=learning_rate\n","    )\n","\n","    early_stopping = EarlyStopping(patience=5, restore_best_weights=True)\n","\n","    model.fit(X_train_seq, y_train_seq,\n","              validation_split=0.2,  # ✅ Validación automática\n","              epochs=epochs,\n","              batch_size=batch_size,\n","              verbose=0,\n","              callbacks=[early_stopping])\n","\n","    y_pred_val = model.predict(X_train_seq[int(len(X_train_seq)*0.8):]).flatten()\n","    y_true_val = y_train_seq[int(len(y_train_seq)*0.8):].flatten()\n","\n","    mae = mean_absolute_error(y_true_val, y_pred_val)\n","    return mae\n","\n","# --- Ejecutar búsqueda con Optuna ---\n","study_lstm = optuna.create_study(direction='minimize')\n","study_lstm.optimize(objective_lstm, n_trials=7, timeout=3600)  # Puedes aumentar a 50+\n","\n","# --- Mostrar mejores hiperparámetros ---\n","print(\"Mejores hiperparámetros encontrados:\")\n","print(study_lstm.best_params)"],"metadata":{"id":"3TP5Q7TXQcxo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n✅ Mejores hiperparámetros encontrados:\")\n","best_params_lstm = study_lstm.best_params\n","for key, value in best_params_lstm.items():\n","    print(f\"{key}: {value}\")\n"],"metadata":{"id":"KDBKbt5KQimx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n🚀 Entrenando el modelo LSTM final con los mejores hiperparámetros...\")\n","\n","final_model_lstm = Sequential()\n","for i in range(best_params_lstm['n_layers']):\n","    return_sequences = i < best_params_lstm['n_layers'] - 1\n","    if i == 0:\n","        final_model_lstm.add(LSTM(best_params_lstm['n_neurons_per_layer'],\n","                                  return_sequences=return_sequences,\n","                                  input_shape=X_train_seq.shape[1:]))\n","    else:\n","        final_model_lstm.add(LSTM(best_params_lstm['n_neurons_per_layer'],\n","                                  return_sequences=return_sequences))\n","    final_model_lstm.add(Dropout(best_params_lstm['dropout_rate']))\n","\n","final_model_lstm.add(Dense(1))\n","\n","final_model_lstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params_lstm['learning_rate']),\n","                         loss='mae')\n","\n","early_stop = EarlyStopping(patience=10, restore_best_weights=True)\n","\n","history_final = final_model_lstm.fit(X_train_seq, y_train_seq,\n","                                     validation_data=(X_test_seq, y_test_seq),\n","                                     epochs=50,\n","                                     batch_size=best_params_lstm['batch_size'],\n","                                     callbacks=[early_stop],\n","                                     verbose=1)\n"],"metadata":{"id":"Jyf0nbMWQm6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# --- 10. Métricas de Evaluación ---\n","y_train_pred_scaled = final_model_lstm.predict(X_train_seq).flatten()\n","y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, 1))\n","y_train_actual = scaler_y.inverse_transform(y_train_seq.reshape(-1, 1))\n","\n","y_test_pred_scaled = final_model_lstm.predict(X_test_seq).flatten()\n","y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled.reshape(-1, 1))\n","y_test_actual = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1))\n","\n","mae_train = mean_absolute_error(y_train_actual, y_train_pred)\n","mse_train = mean_squared_error(y_train_actual, y_train_pred)\n","rmse_train = np.sqrt(mse_train)\n","r2_train = r2_score(y_train_actual, y_train_pred)\n","mape_train = np.mean(np.abs((y_train_actual - y_train_pred) / y_train_actual)) * 100\n","\n","\n","print(\"\\n--- Métricas de Evaluación en el Conjunto de Entrenamiento (Train) ---\")\n","print(f\"MAE (Mean Absolute Error): {mae_train:.4f}\")\n","print(f\"MSE (Mean Squared Error): {mse_train:.4f}\")\n","print(f\"RMSE (Root Mean Squared Error): {rmse_train:.4f}\")\n","print(f\"R2 Score: {r2_train:.4f}\")\n","print(f\"MAPE (Mean Absolute Percentage Error): {mape_train:.4f}%\")\n","\n","\n","mae_test = mean_absolute_error(y_test_actual, y_test_pred)\n","mse_test = mean_squared_error(y_test_actual, y_test_pred)\n","rmse_test = np.sqrt(mse_test)\n","r2_test = r2_score(y_test_actual, y_test_pred)\n","mape_test = np.mean(np.abs((y_test_actual - y_test_pred) / y_test_actual)) * 100\n","\n","print(\"\\n--- Métricas de Evaluación en el Conjunto de Prueba (Test) ---\")\n","print(f\"MAE (Mean Absolute Error): {mae_test:.4f}\")\n","print(f\"MSE (Mean Squared Error): {mse_test:.4f}\")\n","print(f\"RMSE (Root Mean Squared Error): {rmse_test:.4f}\")\n","print(f\"R2 Score: {r2_test:.4f}\")\n","print(f\"MAPE (Mean Absolute Percentage Error): {mape_test:.4f}%\")\n","\n"],"metadata":{"id":"knNrhBdlQrrJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","import joblib\n","from google.colab import files\n","import os\n","\n","# --- Guardar el modelo entrenado ---\n","final_model_lstm.save('lstm2_PM10_model.h5')\n","print(\"✅ Modelo LSTM guardado como 'lstm2_PM10_model.h5'.\")\n","\n","# --- Guardar escaladores ---\n","joblib.dump(scaler_X, 'scaler_X_lstm2.joblib')\n","joblib.dump(scaler_y, 'scaler_y_lstm2.joblib')\n","print(\"✅ Scalers guardados como 'scaler_X_lstm2.joblib' y 'scaler_y_lstm2.joblib'.\")"],"metadata":{"id":"yBU0BHfbVJ_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Descargar archivos generados ---\n","files.download('lstm2_PM10_model.h5')\n","files.download('scaler_X_lstm2.joblib')\n","files.download('scaler_y_lstm2.joblib')\n","\n","# Si generaste un gráfico de predicción y lo guardaste como imagen:\n","# files.download('predicciones_LSTM2.png')\n","\n","print(\"✅ Archivos de modelo y scalers descargados correctamente.\")\n"],"metadata":{"id":"iY58Jh7IQzl4"},"execution_count":null,"outputs":[]}]}